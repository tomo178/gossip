#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ„ãƒ¼ãƒ«2: ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ«ï¼ˆè¨˜äº‹æœ¬æ–‡å–å¾—ç‰ˆï¼‰

æ©Ÿèƒ½:
- detected_topics.json ã‹ã‚‰è¨˜äº‹URLã‚’å–å¾—
- è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã‚’å–å¾—
- è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—
- Gemini APIã§ç”»åƒã‚’åˆ¤å®š
"""

import json
import os
import re
import time
import base64
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import google.generativeai as genai


def load_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    with open('config.json', 'r', encoding='utf-8') as f:
        return json.load(f)


def load_data():
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    config = load_config()
    data_file = config['paths']['data_file']
    
    if os.path.exists(data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []


def save_data(data):
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    config = load_config()
    data_file = config['paths']['data_file']
    
    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def get_og_image(soup, url):
    """og:imageã‚’å–å¾—"""
    og_image = soup.find('meta', property='og:image')
    if og_image and og_image.get('content'):
        image_url = og_image['content']
        if image_url.startswith('http'):
            return image_url
        else:
            return urljoin(url, image_url)
    return None


def get_article_content(soup):
    """è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—"""
    try:
        # è¨˜äº‹æœ¬æ–‡ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œï¼‰
        selectors = [
            '.entry-content',
            '.article-content', 
            '.post-content',
            '.content',
            'article',
            '.article',
            '.main-content',
            '.post-body'
        ]
        
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                text = content.get_text(strip=True, separator='\n')
                if len(text) > 100:  # æœ€å°æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
                    return text
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: bodyå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        body = soup.find('body')
        if body:
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚¿ã‚°ã‚’é™¤å»
            for script in body(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            return body.get_text(strip=True, separator='\n')
        
        return ""
    except Exception as e:
        print(f"    âœ— è¨˜äº‹æœ¬æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return ""


def get_article_images(soup, url):
    """è¨˜äº‹å†…ã®ç”»åƒURLã‚’å–å¾—"""
    try:
        selectors = [
            '.article img',
            'article img',
            '.entry-content img',
            '.post-content img',
            '.content img',
            '.article-content img'
        ]
        
        for selector in selectors:
            imgs = soup.select(selector)
            if imgs:
                image_urls = []
                for img in imgs:
                    src = img.get('src') or img.get('data-src')
                    if src:
                        # Base64ç”»åƒã‚’ã‚¹ã‚­ãƒƒãƒ—
                        if src.startswith('data:'):
                            continue
                        
                        if src.startswith('http'):
                            image_urls.append(src)
                        else:
                            image_urls.append(urljoin(url, src))
                
                return image_urls[:3]  # æœ€å¤§3æš
        
        return []
    except Exception as e:
        print(f"    âœ— è¨˜äº‹ç”»åƒå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def download_image(image_url, save_path):
    """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(image_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # ç”»åƒã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å°10KBï¼‰
        if len(response.content) < 10240:
            return False
            
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        with open(save_path, 'wb') as f:
            f.write(response.content)
            
        return True
    except Exception as e:
        print(f"    âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def init_gemini_api(config):
    """Gemini APIã‚’åˆæœŸåŒ–"""
    genai.configure(api_key=config['gemini_api']['api_key'])
    model = genai.GenerativeModel('gemini-2.0-flash-exp')
    return model


def judge_image_with_gemini(model, image_path, max_retries=3):
    """Gemini APIã§ç”»åƒã‚’åˆ¤å®š"""
    for attempt in range(max_retries):
        try:
            # ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            with open(image_path, 'rb') as f:
                image_data = base64.b64encode(f.read()).decode()
            
            # Gemini APIã«é€ä¿¡
            prompt = """ã“ã®ç”»åƒã‚’è¦‹ã¦ã€ä»¥ä¸‹ã®åŸºæº–ã§åˆ¤å®šã—ã¦ãã ã•ã„ï¼š

âœ… OK: äººç‰©ã®é¡”ãŒã¯ã£ãã‚Šå†™ã£ã¦ã„ã‚‹ï¼ˆèŠ¸èƒ½äººãƒ»æœ‰åäººãƒ»è‘—åäººï¼‰
âŒ NG: é¡”ãŒå°ã•ã„ï¼ˆ10%ä»¥ä¸‹ï¼‰ã€é›†åˆå†™çœŸã€é¢¨æ™¯ãƒ»å»ºç‰©ã®ã¿ã€é¡”ãŒè¦‹ãˆãªã„

ã€ŒOKã€ã¾ãŸã¯ã€ŒNGã€ã®ã¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚"""

            image_part = {
                "mime_type": "image/jpeg",
                "data": image_data
            }
            
            response = model.generate_content([prompt, image_part], 
                request_options={"timeout": 30})
            
            result = response.text.strip().upper()
            
            if "OK" in result:
                return True
            else:
                return False
                
        except Exception as e:
            print(f"    âš  Geminiåˆ¤å®šã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ{attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5
                print(f"    â± {wait_time}ç§’å¾…æ©Ÿä¸­...")
                time.sleep(wait_time)
    
    return False  # å…¨ã¦ã®ãƒªãƒˆãƒ©ã‚¤ãŒå¤±æ•—ã—ãŸå ´åˆ


def process_topic(topic, config, gemini_model):
    """ãƒˆãƒ”ãƒƒã‚¯ã®ç”»åƒã‚’å‡¦ç†"""
    topic_id = topic['id']
    celebrities = topic['celebrities']
    article_url = topic['source_article_url']
    
    print(f"\n  ãƒˆãƒ”ãƒƒã‚¯ID: {topic_id}")
    print(f"  èŠ¸èƒ½äºº: {', '.join(celebrities)}")
    print(f"  è¨˜äº‹URL: {article_url}")
    
    # è¨˜äº‹ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(article_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print(f"  âœ“ è¨˜äº‹ãƒšãƒ¼ã‚¸å–å¾—å®Œäº†")
    except Exception as e:
        print(f"  âœ— è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—
    article_content = get_article_content(soup)
    if article_content:
        topic['article_content'] = article_content
        print(f"  âœ“ è¨˜äº‹æœ¬æ–‡å–å¾—: {len(article_content)}æ–‡å­—")
    else:
        print(f"  âš  è¨˜äº‹æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # og:imageã‚’å–å¾—
    og_image_url = get_og_image(soup, article_url)
    
    # è¨˜äº‹å†…ç”»åƒã‚’å–å¾—
    article_images = get_article_images(soup, article_url)
    
    all_images = []
    if og_image_url:
        all_images.append(('og_image', og_image_url))
    
    for i, img_url in enumerate(article_images, 1):
        all_images.append((f'article_{i}', img_url))
    
    if not all_images:
        print(f"  âš  ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return False
    
    print(f"  ğŸ“¸ ç™ºè¦‹ã—ãŸç”»åƒ: {len(all_images)}æš")
    
    # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»åˆ¤å®š
    approved_images = []
    
    for img_type, image_url in all_images:
        print(f"    ğŸ” {img_type}: {image_url}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        celebrity_names = '_'.join(celebrities)
        safe_names = re.sub(r'[^\w\s-]', '', celebrity_names)
        img_number = len(approved_images) + 2  # 2ã‹ã‚‰é–‹å§‹
        
        save_path = f"./images/{topic_id}_{safe_names}_{img_number}.jpg"
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if download_image(image_url, save_path):
            print(f"    âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            # Geminiåˆ¤å®š
            print(f"    ğŸ¤– Geminiåˆ¤å®šä¸­...")
            if judge_image_with_gemini(gemini_model, save_path):
                print(f"    âœ… æ‰¿èª")
                approved_images.append(save_path)
            else:
                print(f"    âŒ å´ä¸‹ - å‰Šé™¤")
                os.remove(save_path)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(3)
        else:
            print(f"    âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
    
    # çµæœã‚’ä¿å­˜
    if approved_images:
        topic['downloaded_image'] = approved_images[0]
        if len(approved_images) > 1:
            topic['additional_images'] = approved_images[1:]
        topic['status'] = 'image_downloaded'
        
        # é‡è¤‡ç”»åƒå‰Šé™¤ï¼ˆæ‰¿èªãŒ2æšä»¥ä¸Šã®å ´åˆã€1æšç›®ã‚’å‰Šé™¤ï¼‰
        if len(approved_images) >= 2:
            first_image_path = approved_images[0]
            if os.path.exists(first_image_path):
                os.remove(first_image_path)
                print(f"    ğŸ—‘ï¸ é‡è¤‡ç”»åƒã‚’å‰Šé™¤: {os.path.basename(first_image_path)}")
                approved_images = approved_images[1:]
                topic['downloaded_image'] = approved_images[0]
                if len(approved_images) > 1:
                    topic['additional_images'] = approved_images[1:]
                else:
                    topic.pop('additional_images', None)
        
        print(f"  âœ… å®Œäº†: {len(approved_images)}æšã®ç”»åƒã‚’ä¿å­˜")
        return True
    else:
        print(f"  âŒ æ‰¿èªã•ã‚ŒãŸç”»åƒãŒã‚ã‚Šã¾ã›ã‚“")
        return False


def main():
    print("=" * 60)
    print("ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†è¨˜äº‹æœ¬æ–‡å–å¾—ãƒ„ãƒ¼ãƒ« èµ·å‹•")
    print("=" * 60)
    
    # è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    config = load_config()
    data = load_data()
    
    # æ¤œå‡ºæ¸ˆã¿ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
    detected_topics = [t for t in data if t['status'] == 'detected']
    
    if not detected_topics:
        print("\nå‡¦ç†å¯¾è±¡ã®ãƒˆãƒ”ãƒƒã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    print(f"\nå‡¦ç†å¯¾è±¡: {len(detected_topics)}ä»¶ã®ãƒˆãƒ”ãƒƒã‚¯")
    
    # APIåˆæœŸåŒ–
    print("\n[1] APIåˆæœŸåŒ–ä¸­...")
    gemini_model = init_gemini_api(config)
    print("âœ“ åˆæœŸåŒ–å®Œäº†")
    
    # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
    os.makedirs('./images', exist_ok=True)
    
    # å„ãƒˆãƒ”ãƒƒã‚¯ã‚’å‡¦ç†
    print("\n[2] ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†åˆ¤å®šé–‹å§‹...")
    
    success_count = 0
    for topic in detected_topics:
        if process_topic(topic, config, gemini_model):
            success_count += 1
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    print("\n[3] ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
    save_data(data)
    
    print("\n" + "=" * 60)
    print(f"å‡¦ç†å®Œäº†: {success_count}/{len(detected_topics)}ä»¶æˆåŠŸ")
    print("=" * 60)


if __name__ == '__main__':
    main()